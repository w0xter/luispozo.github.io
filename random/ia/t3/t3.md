#INTELIGENCIA ARTIFICIAL: T3, ÁRBOLES DE DECISIÓN.
##DEFINICIONES:
- **Árbol de decisiones:** Representación de una secuencia de preguntas para llegar a una conclusión a partir de una premisa.
![Arbol De Decisón](./img/arbolDecisiones.png)
- **Decisión:** Test sobre un atributo, que deriva a unos u otros nodos dependiendo del valor.
- **Resultado:** Clasificación de un ejemplo dado tras seguir valores de los tests a los atributos.

---

##Arboles de decisión:  
####**Objetivo:** Econtrar el árbol de decisión más compacto.  
######**¿Cómo generear el mejor árbol de decisión a partie de un dataset?**  
![Opciones](./img/2.png)
**Objetivo más asequible**  
Encontrar un árbol suficientemente compacto y suficientemente bueno clasificando.  

---
##Algoritmos de Generación:
####Idea:
######Elegir atributos que mejor partcionen los datos y separar por ellos.
####Algoritmos:
- **ID3:** Aproximación Top-Down basada en la selección de atributos por entropía de *Shannon*
- **C4.5:** Mejora de ID3 con podas para atenuear el *overfitting* y soporte de *varaibles continuas*.
- **Random Forest:** Generación de árboles con selecciones aleatorias de subconjuntos de atributos.
- **CART:** Árboles binarios para la clasificación o regresión con soperte para *variables continuas*.


##ID3: Iterative Dichotmizer Tree.
Fue desarrollado por **Ross Quinlan** en **1979** basa su funcionamiento en la entropía.
Sólo es válido para datos de valores discretos, no acepta datos imprecisos ni ausentes. Tampoco asgura la solución óptima.
###Funcionamiento:
######Dado un conjunto:
1. `if(entropiaGlobal == 0 || atributosDivisibles == 0) EXIT`
2. Seleccionar el **mejor atributo** para dividir la información.
3. Crear tantos nodos como diferente valores tiene l atributo seleccionado. Cada nodo será el subconjunto donde el atributo toma el valor elegido, eliminando ese atributo de la tabla.
4. Aplicar recusrsivamente el algoritmo sobro lso nodos resultantes.

######Dado un Conjunto:
1. `if(entropiaGlobal == 0 || atributosDivisibles == 0) EXIT`
2. Calcular la entropía de todos los atributos del conjunto.
3. Seleccionar el atributo que genere la menor entropía (**La mayor ganancia de información**).
4. Crear tantos nodos cmo diferentes valores tenga el atributo seleccionado. Cada nodo será el subconjunto donde el atributo toma el valor elegido, eliminandose ese atribut de la tabla y construyendo una nueva tabla con las filas que tiene ese valor del atributo.
5. Aplicar recursivamente el algoritmo sobre los nodos resultantes.

####Code:
```PYTHON
def ID3(C):
    if entropía == 0:
        return 0
    else:
        P = Lista de propiedades (columnas) de la tabla C
        EM = EntropíaMedia(Pi) para cada Pi en P
        PM = Propiedad de menor valor de EM
        borrar columna PM de C
        foreach V (valor) en PM:
            Crear rama etiquetada con V
            Crear tabla C’ con las filas de C en las que PM = V
            Llamar recursivamente a ID3(C’)
            end
        end

```

##Entropía de Shannon:
###Cálculo de la entropía de un Conjunto:
######La entropía de Shannon mide la incertidumbre de un conjunto de datos.
- Rango: **[0,log2(n)]** siendo n el número de valores posibles.
- Puede entenderse como el número de bits que hacen falta para identificar un elemento dentro de un conjunto.  
 **-**0 ~ Conjunto Homogéneo: Todos los elementos toman el mismo valor, no hace falta información para saber cuál es el elemento.  
 **-** log2(n) ~ Conjunto Equilibrado: Hace falta la máxima información posible para distinguir cuál es el elemento.  
 
######Cuanto más caótico sea el conjunto mayor será su entropía.  
![Fórmula](img/3.png)  

---

###Cálculo de la entropía media de un atributo:
######Representa cuánto pasa  a valer la entropía de la tabla resultante al particionar un conjunto según los valores de un atributo.  
Primero estudiaremos la entropía para un único valor *v* e una propiedad *D* (Decisora)  con respecto a una variable objetivo *O*.  

![Fórmula](img/5.png)  

**Tan solo tenemos que calcular la entropía del Subconjunto donde *D* = *v* **

---

###Cálculo de la entropía media de una propiedad decisora *D*:
######Conociendo la entropía de cada subconjutno podemos calcular la entropía media.

![Fórmula](img/6.png)  
**La entropía media de un atributo es la media ponderada de los subconjuntos para cada ==v ∈ D==.**

---
###Casos Prácticos:

######Dado el conjunto C:  
![Conjunto C](img/4.1.png)  

- ####Entropía de un conjunto dado:
Calcularemos su entropía a partir de la Probabilidad de cada resultado *(P(rojo) y P(azul))*.
	- 	**P(rojo): ** 10/22  
	- 	**P(azul): ** 12/22  
$$$ E= -[(10/22) * log2(10/22)] - [(12/22) * log2(12/22)]$$$  
**Entropía: ** 0.994  

- ####Entropía media de un atributo:
######¿Que entropia posee el subconjunto donde *A1* = no?
	1. Seleccionamos los atributos que cumplen la condición:  
	**3 Rojo, 10 Azul**
	2. Calculamos la entropía del subcojunto.
		- 	**P(Rojo): ** 3/13
		- 	**P(Azul): ** 10/13

 $$$E = - [(3/13) * log2(3/13)] - [(10/13) * log2(10/13)]$$$
 **Entropía:** 0.780

- ####Entropía media de una propiedad Decisora:
######¿Cuál es la entropía media del atributo *A1*?
	1. Calculamos la entropía del conjunto:
**P(rojo): ** 10/22  
**P(azul): ** 12/22  
$$$ E(total) = -[(10/22) * log2(10/22)] - [(12/22) * log2(12/22)]$$$  
	2. Calculamos la entropía resultante de los subconjuntos obtenidos según los resultados del atributo:
$$$E(A1 = si) = -[(7/9)*log2(7/9)]-[(2/9)*log2(7/9]$$$
$$$E(A1 = no) = -[(3/13)*log2(3/13)]-[(10/13)*log2(10/13)]$$$
**E(A1 = si): ** 0.75  
**E(A1 = no): ** 0.78  
	3. Calculamos la entropía media:
Recordamos que la *entropía media* es una media ponderada tomando como ponderaciones las entropías medias de los valores de un atributo dado (2) y como valores la probabilidad de que se den dichos resultados en el conjunto global. 
$$$EM(A1) = P(A1 = si) * E(A1 = si) + P(A1 = no) * E(A1 = no)$$$
$$$EM(A1) = (9/22 * 0.75) + (13/22 * 0.78)$$$
**EM(A1):** 0.77


- ####Selección de un atributo:
######¿Que atributo es mejor para separar el conjunto?
	1.	Calulamos la entropía del conjunto:
	**E(Total):** 0.99 
	2. Calculamos la entropía media de cada atributo:
		**E(A1):** 0.77  
	**E(A2):** 0.89  

 **El atributo por el que debememos partir el conjunto es *A1* ya que es el que menor entropía media posee de ambos. *(0.77 < 0.89)* **

---

##GANANCIA DE INFORMACIÓN:
#####La *Ganancia de Información* mide cuánto decrece la entropía total al elegir el atributo *A* para dividir el conjunto:  
$$$IG(A) = E(total) - EM(A)$$$
Cuanto menor sea la entropía media de una atributo mayor será la ganancia de información. El objetivo es que la entropía alcance el 0 lo más rápido posible, esto generará un *arbol de estados* más pequeño.
